{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOjVLeZ2C5cj"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQtncD9sU3ko",
        "outputId": "a133f14b-fd1a-4206-8afb-3c102da0225e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "#updating gensim and numpy to run Word2Vec library function\n",
        "!pip install --upgrade gensim\n",
        "!pip3 install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQLXaWWm_aGH",
        "outputId": "67ff36eb-df05-468f-b8b7-b806d6a8a533"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "import re\n",
        "\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# constants\n",
        "sw = stopwords.words('english')\n",
        "#adding more stopwords w.r.t. the data\n",
        "sw.extend(['http', 'https', 'get', 'amp', 'co', 'need', 'like', 'go', 'coronavirus', 'time', 'please','going' ])\n",
        "sw.sort()\n",
        "\n",
        "# constants\n",
        "PATH = 'Corona_NLP_test.txt'\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category = DataConversionWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGNbr9WVmvuO"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "lines = []\n",
        "with open(PATH, 'r') as f:\n",
        "    for l in f:\n",
        "        lines.append(l)\n",
        "\n",
        "#lines['text']=lines['text'].astype(str)\n",
        "\n",
        "# remove new lines\n",
        "lines = [line.rstrip('\\n') for line in lines]\n",
        "\n",
        "# make all characters lower\n",
        "lines = [line.lower() for line in lines]\n",
        "\n",
        "# remove punctuations from each line\n",
        "lines = [line.translate(str.maketrans('', '', string.punctuation)) for line in lines]\n",
        "\n",
        "# tokenize\n",
        "lines = [word_tokenize(line) for line in lines]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdbfFnvN_cZp"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(lines, sw = sw):\n",
        "    '''\n",
        "    The purpose of this function is to remove stopwords from a given array of\n",
        "    lines.\n",
        "\n",
        "    params:\n",
        "        lines (Array / List) : The list of lines you want to remove the stopwords from\n",
        "        sw (Set) : The set of stopwords you want to remove\n",
        "\n",
        "    example:\n",
        "        lines = remove_stopwords(lines = lines, sw = sw)\n",
        "    '''\n",
        "\n",
        "    res = []\n",
        "    for line in lines:\n",
        "        original = line\n",
        "        line = [w for w in line if w not in sw]\n",
        "        if len(line) < 1:\n",
        "            line = original\n",
        "        res.append(line)\n",
        "    return res\n",
        "\n",
        "filtered_lines = remove_stopwords(lines = lines, sw = sw)\n",
        "#filtered_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnBJKKuE_q2w",
        "outputId": "30049265-73fd-4e5c-b7be-39904b60ee33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3521, 100)\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec as w2v\n",
        "\n",
        "w = w2v(\n",
        "    filtered_lines,\n",
        "    min_count=3,\n",
        "    sg = 1,\n",
        "    window=7\n",
        ")\n",
        "\n",
        "#print(w.wv.most_similar('three'))\n",
        "\n",
        "emb_df = (\n",
        "    pd.DataFrame(\n",
        "        [w.wv.get_vector(str(n)) for n in w.wv.key_to_index],\n",
        "        index = w.wv.key_to_index\n",
        "    )\n",
        ")\n",
        "print(emb_df.shape)\n",
        "#emb_df.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXRTOLVHDAQM"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lEDtjOp_t1U"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "#from nltk.corpus import stopwords\n",
        "#from nltk import word_tokenize\n",
        "#from sklearn.decomposition import PCA\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# constants\n",
        "PATH = 'Corona_NLP_test.txt'\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "#lines = pd.read_excel(\"Corona_NLP_test.xlsx\")\n",
        "#lines['Text']=lines['Text'].astype(str)\n",
        "\n",
        "\n",
        "# import data\n",
        "lines = []\n",
        "with open(PATH, 'r') as f:\n",
        "    for l in f:\n",
        "        lines.append(l)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1MXAFAoU_ysk"
      },
      "outputs": [],
      "source": [
        "# Cleaning the texts\n",
        "\n",
        "ps = PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()\n",
        "corpus = []\n",
        "for i in range(len(lines)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', lines[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [wordnet.lemmatize(word) for word in review if not word in set(sw)]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C_aPL2xq_0hT"
      },
      "outputs": [],
      "source": [
        "# Creating the TF-IDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "cv = TfidfVectorizer(ngram_range=(1,1))\n",
        "corpus1 = cv.fit_transform(corpus)\n",
        "avg = corpus1.mean(axis=0)\n",
        "avg = pd.DataFrame(avg, columns=sorted(cv.vocabulary_.keys()))\n",
        "avg=avg.T\n",
        "avg=avg.rename(columns={0:'score'})\n",
        "avg['word']=avg.index\n",
        "avg=avg.sort_values('score', ascending=False)\n",
        "avg=avg[:50]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hwn4voSf_2Xx"
      },
      "outputs": [],
      "source": [
        "cv_bigram = TfidfVectorizer(ngram_range=(3,3))\n",
        "corpus1_bigram= cv_bigram.fit_transform(corpus)\n",
        "avg_bigram=corpus1_bigram.mean(axis=0)\n",
        "avg_bigram= pd.DataFrame(avg_bigram, columns=sorted(cv_bigram.vocabulary_.keys()))\n",
        "avg_bigram=avg_bigram.T\n",
        "avg_bigram=avg_bigram.rename(columns={0:'score'})\n",
        "avg_bigram['word']=avg_bigram.index\n",
        "avg_bigram=avg_bigram.sort_values('score', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfZuhLC1vbSO"
      },
      "outputs": [],
      "source": [
        "unigrams_list=avg['word'].tolist()\n",
        "bigrams_list=avg_bigram['word'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfCZgMhhvzHi"
      },
      "outputs": [],
      "source": [
        "def convert(lst):\n",
        "  return ([item.split() for item in lst])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo-GehCjwD9Y"
      },
      "outputs": [],
      "source": [
        "bigrams_split=convert(bigrams_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krfK0NX4wa7B"
      },
      "outputs": [],
      "source": [
        "check =pd.DataFrame(columns=['topic','subtopic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-Takmf5wkYX"
      },
      "outputs": [],
      "source": [
        "for i in unigrams_list:\n",
        "  counter =0\n",
        "  for j in bigrams_split:\n",
        "    if counter<5 and (i==j[0] or i==j[1] or i==j[2]):\n",
        "      bigram_words=' '.join(j)\n",
        "      check = pd.concat([check, pd.concat([pd.Series(i,name='topic'),pd.Series(bigram_words,name='subtopic')],axis=1)], axis=0)\n",
        "      counter = counter+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03s4bnwDgbhC"
      },
      "source": [
        "# Value of Constraint a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtLBM9YZxzs-",
        "outputId": "db0a3b4d-8e44-48ad-9cad-c761e1620911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        score                 word\n",
            "covid                0.037539                covid\n",
            "food                 0.015422                 food\n",
            "store                0.013970                store\n",
            "grocery              0.011254              grocery\n",
            "stock                0.010669                stock\n",
            "people               0.009588               people\n",
            "supermarket          0.008454          supermarket\n",
            "shopping             0.007522             shopping\n",
            "panic                0.007339                panic\n",
            "online               0.007298               online\n",
            "price                0.006974                price\n",
            "toilet               0.005804               toilet\n",
            "paper                0.005757                paper\n",
            "coronaviruspandemic  0.005400  coronaviruspandemic\n",
            "buying               0.004634               buying\n"
          ]
        }
      ],
      "source": [
        "ans = avg.head(15)\n",
        "print(ans)\n",
        "\"\"\"\"\n",
        "Here, we are manipulating the tf-idf values to fit the bounds of Henon Map by multiplying it with (100)\n",
        "\"\"\"\n",
        "keywords_values = ans['score']*(100)\n",
        "word_index = ans['word']\n",
        "word_index = word_index.to_numpy()\n",
        "constraint_a = keywords_values.to_numpy()\n",
        "#constraint_a\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGOFPFrIDIPK"
      },
      "source": [
        "# Value of Constraint b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMGtS3ioAT_g"
      },
      "outputs": [],
      "source": [
        "constraint_b = []\n",
        "\n",
        "for word in ans['word']:\n",
        "  sum=0\n",
        "  temp = w.wv.most_similar(word, topn = 100)\n",
        "  df_temp=pd.DataFrame(temp, columns=['word','score'])\n",
        "  #print(df_temp)\n",
        "  for value in df_temp['score']:\n",
        "    sum=sum+value\n",
        "  sum=sum/300\n",
        "  constraint_b.append(sum)\n",
        "  #if(len(word)>14):\n",
        "    #print(word,\"\\t\", sum, end =\"\\n\")\n",
        "  #elif(len(word)>6):\n",
        "    #print(word,\"\\t\\t\", sum, end =\"\\n\")\n",
        "  #else:\n",
        "    #print(word,\"\\t\\t\\t\", sum, end =\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytX1sM8FeF62"
      },
      "source": [
        "# Lyapunov Exponent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNtzCF_8d1eF"
      },
      "outputs": [],
      "source": [
        "from numpy import *\n",
        "\n",
        "\n",
        "class HenonMap:\n",
        "    \"\"\" Class to hold the parameters of the Henon Map and evaluate it along its directional derivative.\n",
        "        The default parameters are chosen as the canonical ones in the initialization.\n",
        "        It instantiates a callable object f_df, in such a way that f_df(xy, w)\n",
        "        returns two values f(xy) and df(xy, w), where\n",
        "        f(xy) is the Henon map evaluated at the point xy and\n",
        "        df(xy, w) is the differential of the Henon evaluated at xy in the direction of w.\n",
        "    \"\"\"\n",
        "    def __init__(_, a, b):\n",
        "        _.a, _.b = a, b\n",
        "\n",
        "    def f(_, xy):\n",
        "        x, y = xy\n",
        "        return array([_.a - x ** 2 + _.b * y, x])\n",
        "\n",
        "    def df(_, xy, w):\n",
        "        x, y = xy\n",
        "        j = array([[-2 * x, _.b],\n",
        "                   [1, 0]])\n",
        "        return j @ w\n",
        "\n",
        "    def __call__(_, xy, w):\n",
        "        return _.f(xy), _.df(xy, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9cjhaQa3Syg"
      },
      "outputs": [],
      "source": [
        "from numpy import *\n",
        "from numpy.linalg import *\n",
        "\n",
        "\n",
        "def lyapunov_exponent(f_df, single_initial_condition=None, multiple_initial_conditions=None,\n",
        "                      tol=0.01, max_it=1000, min_it_percentage=0.1):\n",
        "    \"\"\"Numerical approximation of all Lyapunov Exponents of a map.\n",
        "    Ref: Alligood, Kathleen T., Tim D. Sauer, and James A. Yorke. Chaos. Springer New York, 1996.\n",
        "    Parameters\n",
        "    ----------\n",
        "    f_df : callable\n",
        "        The map to be considered for computation of Lyapunov Exponents\n",
        "        f_df (x, w) -> (f(x), df(x, w)), where\n",
        "        f(x) is the map evaluated at the point x and\n",
        "        df(x, w) is the differential of this map evaluated at x in the direction of w.\n",
        "    single_initial_condition : ndarray of shape (n)\n",
        "        Single initial condition to computed the Lyapunov Exponent.\n",
        "    multiple_initial_conditions : ndarray of shape (m,n)\n",
        "        m initial conditions to computed the average Lyapunov Exponent.\n",
        "    tol : float\n",
        "        Tolerance to stop the approximation.\n",
        "    max_it : int\n",
        "        Max numbers of iterations.\n",
        "    min_it_percentage : float, optional\n",
        "        Min number of iterations as a percentage of the max_it.\n",
        "    Returns\n",
        "    -------\n",
        "    : ndarray of shape (n)\n",
        "        The Lyapunov exponents computed associated to a single initial condition or\n",
        "        the average value considering several initial conditions\n",
        "    \"\"\"\n",
        "\n",
        "    if multiple_initial_conditions is not None:\n",
        "        (m, n) = shape(multiple_initial_conditions)\n",
        "        ls = zeros((m, n))\n",
        "        for i in range(m):\n",
        "            ls[i] = lyapunov_exponent(f_df=f_df, single_initial_condition=multiple_initial_conditions[i],\n",
        "                                      tol=tol, max_it=max_it, min_it_percentage=min_it_percentage)\n",
        "        return apply_along_axis(lambda v: mean(v), 0, ls)\n",
        "\n",
        "    elif single_initial_condition is None:\n",
        "        raise Exception('Either single_initial_condition or multiple_initial_conditions must be provided.')\n",
        "\n",
        "    n = len(single_initial_condition)\n",
        "    x = single_initial_condition\n",
        "    w = eye(n)\n",
        "    h = zeros(n)\n",
        "    trans_it = int(max_it * min_it_percentage)\n",
        "    l = -1\n",
        "\n",
        "    for i in range(0, max_it):\n",
        "        x_next, w_next = f_df(x, w)\n",
        "        w_next = orthogonalize_columns(w_next)\n",
        "\n",
        "        h_next = h + log_of_the_norm_of_the_columns(w_next)\n",
        "        l_next = h_next / (i + 1)\n",
        "\n",
        "        if i > trans_it and norm(l_next - l) < tol:\n",
        "            return sort(l_next)\n",
        "\n",
        "        h = h_next\n",
        "        x = x_next\n",
        "        w = normalize_columns(w_next)\n",
        "        l = l_next\n",
        "\n",
        "    return (-1,-1)\n",
        "    raise Exception('Lyapunov Exponents computation did no convergence' +\n",
        "                    ' at ' + str(single_initial_condition) +\n",
        "                    ' with tol=' + str(tol) +\n",
        "                    ' max_it=' + str(max_it) +\n",
        "                    ' min_it_percentage=' + str(min_it_percentage))\n",
        "\n",
        "\n",
        "def orthogonalize_columns(a):\n",
        "    q, r = qr(a)\n",
        "    return q @ diag(r.diagonal())\n",
        "\n",
        "\n",
        "def normalize_columns(a):\n",
        "    return apply_along_axis(lambda v: v / norm(v), 0, a)\n",
        "\n",
        "\n",
        "def log_of_the_norm_of_the_columns(a):\n",
        "    return apply_along_axis(lambda v: log(norm(v)), 0, a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9MEY6LC3WzZ",
        "outputId": "d67e5033-8adc-47c7-ebee-5027b0f690c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-39-a07c66d0d108>:84: RuntimeWarning: divide by zero encountered in log\n",
            "  return apply_along_axis(lambda v: log(norm(v)), 0, a)\n",
            "<ipython-input-39-a07c66d0d108>:80: RuntimeWarning: invalid value encountered in divide\n",
            "  return apply_along_axis(lambda v: v / norm(v), 0, a)\n",
            "<ipython-input-38-583cf6318a15>:17: RuntimeWarning: overflow encountered in double_scalars\n",
            "  return array([_.a - x ** 2 + _.b * y, x])\n",
            "<ipython-input-38-583cf6318a15>:23: RuntimeWarning: invalid value encountered in matmul\n",
            "  return j @ w\n",
            "<ipython-input-39-a07c66d0d108>:80: RuntimeWarning: divide by zero encountered in divide\n",
            "  return apply_along_axis(lambda v: v / norm(v), 0, a)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 , [-1. -1.]\n",
            "1 , [-1. -1.]\n",
            "2 , [-1.68416455  0.37497413]\n",
            "3 , [-1.4835986   0.19575036]\n",
            "4 , [-1.29720292  0.07176951]\n",
            "5 , [-0.7156278  -0.43114817]\n",
            "6 , [-1.02988252 -0.16175524]\n",
            "7 , [-0.73261749 -0.59907005]\n",
            "8 , [-0.67679417 -0.54517721]\n",
            "9 , [-0.76381987 -0.61995599]\n",
            "10 , [-0.61865679 -0.50783496]\n",
            "11 , [-0.72383802 -0.55228756]\n",
            "12 , [-0.72279111 -0.54817596]\n",
            "13 , [-0.91520318 -0.42517845]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14 , [-1.02557867 -0.20209051]\n"
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "from numpy import *\n",
        "from numpy.linalg import *\n",
        "\n",
        "#HENON_MAP_REFERENCE_SOLUTION = array([-1.61, 0.41])\n",
        "#TEST_TOLERANCE = 0.01\n",
        "x_points, y_points = mgrid[-1:1:0.25, -1:1:0.25]\n",
        "HENON_MAP_INITIAL_CONDITIONS = column_stack((x_points.ravel(), y_points.ravel()))\n",
        "\n",
        "\n",
        "# Ref: Alligood, Kathleen T., Tim D. Sauer, and James A. Yorke. Chaos. Springer New York, 1996.\n",
        "#range bound = (1.1,0.4), (1.8,0.1); always chaotic at (1.4,0.3)\n",
        "henon_values = []\n",
        "for i in range(0,15):\n",
        "  HENON_MAP = HenonMap(constraint_a[i], constraint_b[i])\n",
        "  a =  lyapunov_exponent(HENON_MAP, multiple_initial_conditions=HENON_MAP_INITIAL_CONDITIONS)\n",
        "  print(i, ',',a)\n",
        "  henon_values.append(a)\n",
        "\n",
        "\n",
        "class LyapunovExponentsTestCase(unittest.TestCase):\n",
        "    def henon_map_test(_):\n",
        "        l = lyapunov_exponent(HENON_MAP, multiple_initial_conditions=HENON_MAP_INITIAL_CONDITIONS)\n",
        "        _.assertEqual(norm(HENON_MAP_REFERENCE_SOLUTION - l, inf) < TEST_TOLERANCE, True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUl9vOtdyR1j",
        "outputId": "b2f04ba5-90ad-491c-fab8-ce0afe81dc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "covid \t\t\t [-1. -1.]\n",
            "food \t\t\t [-1. -1.]\n",
            "store \t\t\t [-1.68416455  0.37497413]\n",
            "grocery \t\t [-1.4835986   0.19575036]\n",
            "stock \t\t\t [-1.29720292  0.07176951]\n",
            "people \t\t\t [-0.7156278  -0.43114817]\n",
            "supermarket \t\t [-1.02988252 -0.16175524]\n",
            "shopping \t\t [-0.73261749 -0.59907005]\n",
            "panic \t\t\t [-0.67679417 -0.54517721]\n",
            "online \t\t\t [-0.76381987 -0.61995599]\n",
            "price \t\t\t [-0.61865679 -0.50783496]\n",
            "toilet \t\t\t [-0.72383802 -0.55228756]\n",
            "paper \t\t\t [-0.72279111 -0.54817596]\n",
            "coronaviruspandemic \t [-0.91520318 -0.42517845]\n",
            "buying \t\t\t [-1.02557867 -0.20209051]\n"
          ]
        }
      ],
      "source": [
        "#print(henon_values)\n",
        "for i in range(15):\n",
        "  if(len(ans['word'][i])>14):\n",
        "    print(ans['word'][i], \"\\t\", henon_values[i], end = \"\\n\")\n",
        "  elif(len(ans['word'][i])>6):\n",
        "    print(ans['word'][i], \"\\t\\t\", henon_values[i], end = \"\\n\")\n",
        "  else:\n",
        "    print(ans['word'][i], \"\\t\\t\\t\", henon_values[i], end = \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdA4-SoI_oNT"
      },
      "source": [
        "# Chaotic and Non-chaotic words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frNtqOH_1eiY",
        "outputId": "a034228b-794e-4cd8-8ade-28c8d05e17fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chaotic Words:\n",
            "\n",
            "store\n",
            "grocery\n",
            "stock\n",
            "\n",
            "\n",
            "Non-Chaotic Words:\n",
            "\n",
            "covid\n",
            "food\n",
            "people\n",
            "supermarket\n",
            "shopping\n",
            "panic\n",
            "online\n",
            "price\n",
            "toilet\n",
            "paper\n",
            "coronaviruspandemic\n",
            "buying\n"
          ]
        }
      ],
      "source": [
        "chaotic_words=[]\n",
        "non_chaotic_words =[]\n",
        "for i in range(15):\n",
        "  if(henon_values[i][1]>0):\n",
        "    chaotic_words.append(i)\n",
        "  else:\n",
        "    non_chaotic_words.append(i)\n",
        "\n",
        "print(\"Chaotic Words:\\n\")\n",
        "for w in chaotic_words:\n",
        "  a = ans.loc[word_index[w]]\n",
        "  print(a['word'])\n",
        "\n",
        "print(\"\\n\\nNon-Chaotic Words:\\n\")\n",
        "for w in non_chaotic_words:\n",
        "  a = ans.loc[word_index[w]]\n",
        "  print(a['word'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}